# Цели и задачи исследования
Цель: Найти лучшую модель, которая будет предсказывать рыночную цену автомобиля
Задачи:
* 1. Прочитать данные и провести предобработку
* 2. Подготовить выборки для обучения моделей
* 3. Обучить модели
* 4. Проанализировать результаты (необходимо учесть время обучения, время предсказания и качество модели)

## Загрузите данные
pip install catboost
pip install lightgbm
pip install xgboost
pip install ydata-profiling
import os
import time

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import pandas_profiling

import lightgbm as lgb
from catboost import CatBoostRegressor, Pool
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.dummy import DummyRegressor

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
pth1 = 'autos.csv'
pth2 = '/datasets/autos.csv'

if os.path.exists(pth1):
    df = pd.read_csv(pth1)
elif os.path.exists(pth2):
    df = pd.read_csv(pth2)
else:
    print('Something is wrong')
## Изучите данные. Заполните пропущенные значения и обработайте аномалии в столбцах. Если среди признаков имеются неинформативные, удалите их.
print(df.info())
display(df.head())
Видим, что числовые данные не содержат пропусков, все пропуски в категориальных столбцах, поэтому заменим все пропущенные значения в них на заглушку "No data"
categories = ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Repaired']
df[categories] = df[categories].fillna('No data')
Проверяем
print(df.info())
pandas_profiling.ProfileReport(df)
Видим много нулей в мощности и месяце регистрации. Заменим их заглушкой - ценой-медианой, потому что потеря такого количества данных - очень много.
null_data = ['Power', 'RegistrationMonth']
for x in range(len(null_data)):
    df.loc[df[null_data[x]]==0, null_data[x]] = df[null_data[x]].median()
Нули в цене уберём. Их хоть и целых 10 тысяч, но на весь объём их меньше 3 процентов.
df = df[df['Price']!=0]
pandas_profiling.ProfileReport(df)
Нулей больше нет
duplicateRows = df[df.duplicated()]
duplicateRows.info()
Осталось 4 дубликата, которые мы смело можем выбросить
df = df.drop_duplicates()
df.info()
Итак, изавились от дубликатов.
## Подготовка выборки для обучения моделей.
Прежде всего, уберём даты из признаков. Они нам не понадобятся
Также уберём PostalCode. Вряд ли этот показатель говорит даже о реальной географии продажи.
NumberOfPictures содержит одно значение, оно нам ничего не даст.
df = df.drop(columns=['DateCrawled', 'DateCreated', 'LastSeen', 'PostalCode', 'NumberOfPictures'])
<div class="alert alert-block alert-info">
<b>Комментарий студента 2:</b> Да, не обратил внимание, что тут одно значение на все строчки
</div>
Поделим её с соотношении 60-20-20. 60% процентов на тренировочную, по 20% на валидационную и тестовую. 
features = df.drop(columns=['Price'])
target = df['Price']
    
features_tr, features_tv, target_train, target_tv = train_test_split(
    features, target, test_size= 0.4, random_state=12345) 

features_t, features_val, target_test, target_valid = train_test_split(
    features_tv, target_tv, test_size= 0.5, random_state=12345) 
cat_cols = ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Brand','Repaired']
ohe = OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore')
ohe.fit(features_tr[cat_cols])
features_train = features_tr.drop(columns=cat_cols).copy()
features_train[ohe.get_feature_names_out()] = ohe.transform(features_tr[cat_cols])

features_valid = features_val.drop(columns=cat_cols).copy()
features_valid[ohe.get_feature_names_out()] = ohe.transform(features_val[cat_cols])

features_test = features_t.drop(columns=cat_cols).copy()
features_test[ohe.get_feature_names_out()] = ohe.transform(features_t[cat_cols])
## Обучаем разные модели.
### Прописываем функцию
def get_results(test_models, index):
    test_model = test_models[0] # выделяем модель
    try: # если это не катбуст и у нас две выборки, а не пул, то мы выделяем x и y для обучения модели
        if len(test_models[1]) > 1:
            new_fit=test_models[1]
            x=new_fit[0]
            y=new_fit[1]
            start = time.time()
            model = test_model.fit(x,y)
            end = time.time()
            learning_time = end-start
    except: # если это пул, что передаём только его
        df_fit = test_models[1]
        start = time.time()
        model = test_model.fit(df_fit)
        end = time.time()
        learning_time = end-start   
    start = time.time()
    predictions_valid = model.predict(features_valid)
    end = time.time()
    predictions_time = end-start
    rmse = mean_squared_error(target_valid, predictions_valid) ** 0.5
    
    df_results.loc[index, 'model'] = name_models[index]
    df_results.loc[index, 'rmse'] = rmse
    df_results.loc[index, 'learning_time_absolute'] = learning_time
    df_results.loc[index, 'predictions_time_absolute'] = predictions_time
    
    return 
    
### Знакомство с моделями
Создадим фрейм, куда будем добавлять результаты моделей
df_results = pd.DataFrame()
models = ['catboost', 'lgbm', 'xgb', 'random_forest']
name_models = ['catboost default', 'catboost iterations=1500', 
               'lgbm default', 'lgbm rf', 'lgbm dart', 
               'xgb default', 'xgb dart', 
               'random_forest default']
Создаём пулы для прогнозирования в catboosting
pool_train = Pool(features_train, target_train)
pool_valid = Pool(features_valid, target_valid)
Создаём сборник всех моделей
model_fit = [
    [CatBoostRegressor(random_state=12345), pool_train],
    [CatBoostRegressor(random_state=12345, iterations=1500), pool_train],
    [lgb.LGBMRegressor(random_state=12345), [features_train, target_train]],
    [lgb.LGBMRegressor(random_state=12345, boosting_type='rf', n_estimators=100, subsample_freq=1, bagging_fraction=0.5, feature_fraction=0.5), [features_train, target_train]],
    [lgb.LGBMRegressor(random_state=12345, boosting_type='dart'), [features_train, target_train]],
    [xgb.XGBRegressor(random_state=12345), [features_train, target_train]],
    [xgb.XGBRegressor(random_state=12345, boosting_type='dart'), [features_train, target_train]],
    [RandomForestRegressor(random_state=12345), [features_train, target_train]]
]
for index in range(len(model_fit)):
        get_results(model_fit[index], index)
df_results.sort_values(by='rmse')
df_results.sort_values(by='learning_time_absolute')
df_results.sort_values(by='predictions_time_absolute')
Итак, мы видим, что по метрикам RMSE и время предсказания доминирует catboost,  по времени обучения lgbm. Непхолие показатели RMSE у случайного леса, но слишком уж долгое время обучения и прогноза
В принципе все лучшие модели - дефолтные. У catboost RMSE 1500 итераций превосходят базовую конфигурацию, но незначительно, а проигрыш во времени слишком большой. Во времени предсказания различия конфигураций слишком несущественные, чтобы их учитывать при выборе модели.
### Перебор параметров
Оставляем только дефолтные модели
for x in range(len(df_results['model'])):
    if 'default' not in df_results.loc[x,'model']:
        df_results = df_results.drop(labels=[x],axis=0)
df_best_results = df_results.reset_index(drop=True)
df_best_results
#### Создадим функцию для перебора параметров
def selection(best_model, index, final_depth, final_est):
    test_model = best_model[0]
    
    best_est = 0
    best_depth = 0
    best_rmse = df_best_results.loc[index, 'rmse']
    best_learning_time = df_best_results.loc[index, 'learning_time_absolute']
    best_predictions_time = 0 
    
    if index>0:
        new_fit=best_model[1]
        x=new_fit[0]
        y=new_fit[1]
        for depth in range (1, final_depth, 5):
            for est in range (1, final_est, 20):
                start = time.time()
                model = test_model(random_state = 12345, max_depth=depth, n_estimators = est).fit(x,y)
                end = time.time()
                learning_time = end-start
                start = time.time()
                predictions_valid = model.predict(features_valid)
                end = time.time()
                predictions_time = end-start
                rmse = mean_squared_error(target_valid, predictions_valid) ** 0.5
                if rmse < best_rmse and learning_time < best_learning_time:
                    best_est = est
                    best_depth = depth
                    best_rmse = rmse
                    best_learning_time = learning_time 
                    best_predictions_time = predictions_time

    elif index==0: 
        df_fit = best_model[1]
        for depth in range (1, final_depth, 5):
            for est in range (1, final_est, 20):
                start = time.time()
                model = test_model(random_state = 12345, max_depth=depth, n_estimators = est).fit(df_fit)
                end = time.time()
                learning_time = end-start
                start = time.time()
                predictions_valid = model.predict(features_valid)
                end = time.time()
                predictions_time = end-start
                rmse = mean_squared_error(target_valid, predictions_valid) ** 0.5
                if rmse < best_rmse and learning_time < best_learning_time:
                    best_est = est
                    best_depth = depth
                    best_rmse = rmse
                    best_learning_time = learning_time 
                    best_predictions_time = predictions_time
      
    
    df_best_results.loc[len(best_models)+index, 'model'] = '{} d={} & e={}'.format(models[index], best_depth, best_est)
    df_best_results.loc[len(best_models)+index, 'rmse'] = best_rmse
    df_best_results.loc[len(best_models)+index, 'learning_time_absolute'] = best_learning_time
    df_best_results.loc[len(best_models)+index, 'predictions_time_absolute'] = best_predictions_time
    
    return
Создаём список лучших моделей
best_models = [
    [CatBoostRegressor, pool_train],
    [lgb.LGBMRegressor, [features_train, target_train]],
    [xgb.XGBRegressor, [features_train, target_train]],
    [RandomForestRegressor, [features_train, target_train]]
]
final_depth, final_est
Создаём цикл с перебором. Параметров для более быстрых моделей больше, для медленных, соответсвенно, меньше.
for index in range(len(best_models)):
    print(index)
    if index < 2:
        final_depth = 17
        final_est = 102
    if index == 2:
        final_depth = 12
        final_est = 102
    if index == 3:
        final_depth = 6
        final_est = 62
    selection(best_models[index], index, final_depth, final_est)
## Проанализируем время обучения, время предсказания и качество моделей
df_best_results.sort_values(by='rmse')
Отсекаем строки, где лучшую модель не удалось найти
for x in range(len(df_best_results['model'])):
    if df_best_results.loc[x,'predictions_time_absolute']==0:
        df_best_results=df_best_results.drop(labels=[x], axis=0)
df_best_results.reset_index(drop=True)
Посмотрим на сводные данные
df_best_results.sort_values(by='rmse')
df_best_results.sort_values(by='learning_time_absolute')
df_best_results.sort_values(by='predictions_time_absolute')
По двум из трёх показателей побеждает catboost с глубиной 11 и 101 деревом, тем более побеждает в самом важной метрике - RMSE.
### Доработка лучшей модели
Поскольку мы взяли большие шаги на поиске, попробуем поискать рядом с лучшими параметрами более тщательно
best_est = 0
best_depth = 0
best_rmse = df_best_results.loc[4, 'rmse']
best_learning_time = df_best_results.loc[4, 'learning_time_absolute']
best_predictions_time = 0 
for depth in range (10, 12):
    for est in range (98, 105, 2):
        start = time.time()
        model = CatBoostRegressor(random_state=12345, n_estimators=est, max_depth=depth).fit(pool_train)
        end = time.time()
        learning_time = end-start
        start = time.time()
        predictions_valid = model.predict(features_valid)
        end = time.time()
        predictions_time = end-start
        rmse = mean_squared_error(target_valid, predictions_valid) ** 0.5
        if rmse < best_rmse and learning_time < best_learning_time:
            best_est = est
            best_depth = depth
            best_rmse = rmse
            best_learning_time = learning_time 
            best_predictions_time = predictions_time
            

print(f"RMSE наилучшей модели на валидационной выборке при глубине {best_depth} и количестве деревьев {best_est} : {best_rmse}")
print(f"Время обучения наилучшей модели: {best_learning_time}, время предсказания: {best_predictions_time}")
Действительно, с показателем в 102 дерева числа чуть лучше
### Тест
Теперь проверим лучшую модель на тестовой выборке.
start = time.time()
model = CatBoostRegressor(random_state=12345, n_estimators=102, max_depth=11).fit(pool_train)
end = time.time()
learning_time = end-start
print(f'Время обучения: {learning_time}')
start = time.time()
predictions_test = model.predict(features_test)
end = time.time()
predictions_time = end-start
print(f'Время предсказания: {predictions_time}')
rmse = mean_squared_error(target_test, predictions_test) ** 0.5
print('RMSE:', rmse)
На тесте результаты по RMSE даже чуть лучше, а по времени чуть хуже. Однако результаты всё равно хорошие.
### Dummy модель
Посмотрим показатели медианы
dmb = DummyRegressor(strategy='median')
dmb.fit(features_train, target_train)
predictions_valid = dmb.predict(features_valid)
rmse = mean_squared_error(target_valid, predictions_valid) ** 0.5
print('Dummy_RMSE:', rmse)
Они выше даже самых плохих показателей нашего исследования.
# Итоговый вывод
В ходе исследование я рассмотрел датасет с данными о продажах автомобилей. В нём было много пропусков в категориальных столюцах, которые я заполнил заглушкой "No data" 
Заглушкой медианой были заполнены нули в мощности и месяце регистрации. Оставшиеся 4 дубликата были удалены.
Нули в цене были выброшены из выборки. Аномалии в целевом признаке нам не нужны, а таковых меньше 3 процентов.
Мною были удалены столбцы с датами и неинформативная колонка "PostalCode". Остальные категориальные признаки были обработаны методом OHE.
Исходная выборка была разделена на тренировочную, валидационную и тестовую в соотношении 60-20-20.
Было проверено 4 разные модели: catboost, lightgbm, xgboost, randomforest.
Чемпион по RMSE и времени предсказания catboost с глубиной 11 и количеством деревьев - 98. Во времени обучения лучшие показатели продемонстрировала lightgbm.
Если сравнивать относительно, то RMSE у catboost на пять процентов лучше, чем lightgbm, а время прогнозирования уступает в 4 раза. Однако на в абсолютных показателях на таких числах разница между четырьмя секундами и одной не столь существенна, а критерий качества представляется приоритетным показателям.
Поэтому рекомендую использовать для прогнозирования цены автомобиля catboost с глубиной 11 и количеством деревьев - 98 как самую точную и одну из самых быстрых моделей.
